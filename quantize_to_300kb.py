#!/usr/bin/env python3
"""
Quantize model to <300KB while preserving accuracy.
Uses FP16 + quantization (no pruning).
"""

from pathlib import Path
import torch
import torch.nn as nn
import shutil
import json

# Will be determined from metadata
MODEL_PATH = None
BACKUP_PATH = None


def main():
    print("="*60)
    print("Quantizing Model to <300KB")
    print("="*60)
    
    # Load model path from metadata
    print("\nLoading model metadata...")
    from brainstorm.ml.utils import import_model_class
    
    metadata_path = Path("model_metadata.json")
    if not metadata_path.exists():
        print(f"âŒ model_metadata.json not found.")
        return
    
    with open(metadata_path) as f:
        metadata = json.load(f)
    
    # Get actual model path from metadata
    model_path_str = metadata.get("model_path", "model.pt")
    global MODEL_PATH, BACKUP_PATH
    MODEL_PATH = Path(model_path_str)
    BACKUP_PATH = Path(f"{model_path_str}.backup")
    
    if not MODEL_PATH.exists():
        print(f"âŒ Model not found: {MODEL_PATH}")
        return
    
    # Backup
    shutil.copy(MODEL_PATH, BACKUP_PATH)
    print(f"âœ“ Backed up to {BACKUP_PATH}")
    
    orig_size = MODEL_PATH.stat().st_size
    print(f"\nOriginal size: {orig_size / 1024:.1f} KB ({orig_size / (1024*1024):.2f} MB)")
    
    # Load model
    print("\nLoading model...")
    model_class = import_model_class(metadata["import_string"])
    model = model_class.load()
    model.eval()
    model.to("cpu")
    
    print(f"  Model file: {MODEL_PATH}")
    
    # Load checkpoint to preserve structure
    checkpoint = torch.load(MODEL_PATH, weights_only=False, map_location="cpu")
    
    # Step 1: Convert to FP16 (minimal accuracy loss)
    print("\n" + "="*60)
    print("Step 1: Converting to FP16...")
    print("="*60)
    
    with torch.no_grad():
        for param in model.parameters():
            if param.dtype == torch.float32:
                param.data = param.data.half()
        for buffer in model.buffers():
            if buffer.dtype == torch.float32:
                buffer.data = buffer.data.half()
    
    print("âœ“ Converted all weights to FP16")
    
    # Step 2: Quantize Linear layers (minimal accuracy impact)
    print("\n" + "="*60)
    print("Step 2: Quantizing Linear layers...")
    print("="*60)
    
    torch.backends.quantized.engine = 'qnnpack'
    
    # Try new API first, fallback to old
    try:
        qmodel = torch.ao.quantization.quantize_dynamic(
            model,
            {nn.Linear},
            dtype=torch.qint8,
        )
        print("âœ“ Quantized Linear layers (using torch.ao.quantization)")
    except (AttributeError, ImportError):
        import torch.quantization as tq
        qmodel = tq.quantize_dynamic(
            model,
            {nn.Linear},
            dtype=torch.qint8,
        )
        print("âœ“ Quantized Linear layers (using torch.quantization)")
    
    # Step 3: Save in compact checkpoint format
    print("\n" + "="*60)
    print("Step 3: Saving compressed checkpoint...")
    print("="*60)
    
    # Update config
    config = checkpoint["config"].copy()
    config["quantized"] = True
    config["fp16"] = True
    
    # Compress stats and channel_map arrays
    compressed_checkpoint = {
        "config": config,
        "classes": checkpoint["classes"],
        "state_dict": qmodel.state_dict(),
    }
    
    # Compress stats to float16
    if "stats" in checkpoint:
        stats = checkpoint["stats"]
        compressed_checkpoint["stats"] = {
            "mean": stats["mean"].astype("float16") if hasattr(stats["mean"], "astype") else stats["mean"],
            "std": stats["std"].astype("float16") if hasattr(stats["std"], "astype") else stats["std"],
        }
    
    # Compress channel_map to smaller int types
    if "channel_map" in checkpoint:
        cm = checkpoint["channel_map"]
        compressed_checkpoint["channel_map"] = {
            "channel_idx": cm["channel_idx"].astype("int16") if hasattr(cm["channel_idx"], "astype") else cm["channel_idx"],
            "x": cm["x"].astype("int8") if hasattr(cm["x"], "astype") else cm["x"],
            "y": cm["y"].astype("int8") if hasattr(cm["y"], "astype") else cm["y"],
        }
    
    # Save with compact serialization
    torch.save(
        compressed_checkpoint,
        MODEL_PATH,
        _use_new_zipfile_serialization=True,
        pickle_protocol=4
    )
    
    final_size = MODEL_PATH.stat().st_size
    change = (final_size - orig_size) / orig_size * 100
    target_bytes = 300 * 1024
    
    print(f"\n" + "="*60)
    print("Results")
    print("="*60)
    print(f"  Original: {orig_size / 1024:.1f} KB")
    print(f"  Quantized: {final_size / 1024:.1f} KB ({change:+.1f}% change)")
    print(f"  Target: <300 KB ({target_bytes / 1024:.0f} KB)")
    
    if final_size < target_bytes:
        print(f"  âœ… Model size is below 300 KB target!")
    else:
        reduction_needed = (final_size - target_bytes) / 1024
        print(f"  âš ï¸  Model size is {reduction_needed:.1f} KB above target")
        print(f"  ðŸ’¡ Consider: Further metadata compression or lighter quantization")
    
    print(f"\n  ðŸ’¡ Accuracy preservation:")
    print(f"     - FP16: ~50% size reduction, minimal accuracy loss (<1%)")
    print(f"     - Quantization: Additional ~25% reduction, minimal accuracy impact (1-2%)")
    print(f"     - No pruning: Preserves accuracy (pruning was removed)")
    
    print(f"\n  Backup: {BACKUP_PATH}")
    print(f"  âœ“ Model saved to: {MODEL_PATH}")


if __name__ == "__main__":
    main()
